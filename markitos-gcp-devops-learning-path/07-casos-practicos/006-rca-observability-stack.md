# ‚òÅÔ∏è Caso Pr√°ctico: RCA de Lentitud en App con el Stack de Observabilidad

## üìë √çndice

* [üß≠ Escenario del Problema](#-escenario-del-problema)
* [üõ†Ô∏è Herramientas Utilizadas](#Ô∏è-herramientas-utilizadas)
* [üïµÔ∏è‚Äç‚ôÇÔ∏è Proceso de An√°lisis de Causa Ra√≠z (RCA)](#Ô∏è-proceso-de-an√°lisis-de-causa-ra√≠z-rca)
* [üî¨ Laboratorio Pr√°ctico (Simulaci√≥n y An√°lisis)](#-laboratorio-pr√°ctico-simulaci√≥n-y-an√°lisis)
* [üí° Lecciones Aprendidas](#-lecciones-aprendidas)
* [üßæ Resumen](#-resumen)
* [‚úçÔ∏è Firma](#-firma)

---

## üß≠ Escenario del Problema

Los usuarios de una aplicaci√≥n web de comercio electr√≥nico, que se ejecuta en Cloud Run, reportan que la p√°gina para ver los detalles de un producto a veces tarda mucho en cargar (m√°s de 5 segundos), mientras que otras p√°ginas funcionan con normalidad. El equipo de SRE recibe una alerta de **Cloud Monitoring** que indica que la latencia del 95 percentil para el servicio de Cloud Run ha superado el umbral definido en el SLO.

**Objetivo:** Utilizar el stack de observabilidad de Google Cloud (Monitoring, Logging, Trace) para identificar la causa ra√≠z de la latencia, desde el s√≠ntoma inicial hasta el problema de fondo, y proponer una soluci√≥n.

---

## üõ†Ô∏è Herramientas Utilizadas

1.  **Cloud Monitoring (Metrics):** Para la visi√≥n general del rendimiento. Usaremos el **Metrics Explorer** y los **Dashboards** para visualizar la latencia, el n√∫mero de peticiones y la utilizaci√≥n de recursos.
2.  **Cloud Trace:** Para descomponer una petici√≥n individual en sus componentes (spans) y ver d√≥nde se est√° gastando el tiempo (ej. llamada a la base de datos, a una API externa, etc.).
3.  **Cloud Logging (Logs):** Para obtener contexto detallado sobre lo que estaba ocurriendo en la aplicaci√≥n en el momento de una petici√≥n lenta. Usaremos el **Logs Explorer** y correlacionaremos los logs con las trazas.

---

## üïµÔ∏è‚Äç‚ôÇÔ∏è Proceso de An√°lisis de Causa Ra√≠z (RCA)

El proceso sigue un embudo, desde lo m√°s general a lo m√°s espec√≠fico.

1.  **MACRO (Monitoring): ¬øQu√© est√° pasando?**
    *   Se empieza en el dashboard de Cloud Monitoring o en el Metrics Explorer.
    *   Se observa la m√©trica de latencia (`run.googleapis.com/request_latencies`) del servicio de Cloud Run.
    *   Se confirma el pico de latencia reportado por la alerta. Se correlaciona con otras m√©tricas: ¬øsubi√≥ tambi√©n el n√∫mero de peticiones? ¬øLa utilizaci√≥n de CPU o memoria?
    *   En este escenario, vemos que la latencia sube, pero el tr√°fico y la CPU no han variado significativamente. Esto sugiere que el problema no es de sobrecarga, sino de eficiencia en el procesamiento de ciertas peticiones.

2.  **MESO (Trace): ¬øD√≥nde est√° pasando?**
    *   Desde el dashboard de Monitoring, se puede saltar directamente a las trazas asociadas a un per√≠odo de tiempo problem√°tico.
    *   En Cloud Trace, se filtra por las peticiones con mayor latencia (ej. `latency > 5s`).
    *   Se selecciona una traza de una petici√≥n lenta. El diagrama de Gantt de la traza muestra varios "spans" o tramos. Se busca el span que consume la mayor parte del tiempo.
    *   En nuestro escenario, la traza muestra un span principal de 5 segundos, y dentro de √©l, un span de 4.8 segundos etiquetado como `CloudSQL: SELECT * FROM products WHERE id=...`. **Hemos aislado el problema a una consulta a la base de datos.**

3.  **MICRO (Logging): ¬øPor qu√© est√° pasando?**
    *   Cada traza est√° correlacionada con los logs de la petici√≥n correspondiente. Desde Cloud Trace, se puede hacer clic en "Ver Logs".
    *   Esto abre el Logs Explorer con un filtro predefinido (`trace=...`) que muestra solo los logs de esa petici√≥n lenta.
    *   Se revisan los logs de la aplicaci√≥n. Podr√≠amos encontrar un log que diga: `Query for product ID 12345 took 4850ms. Query plan shows a full table scan.`
    *   **¬°Bingo!** La consulta a la base de datos est√° realizando un escaneo completo de la tabla (`full table scan`) en lugar de usar un √≠ndice, lo que es extremadamente ineficiente para tablas grandes.

**Conclusi√≥n del RCA:** La causa ra√≠z de la latencia es una consulta SQL ineficiente a la base de datos, probablemente debido a la falta de un √≠ndice en la columna `id` de la tabla `products`.

---

## üî¨ Laboratorio Pr√°ctico (Simulaci√≥n y An√°lisis)

Este laboratorio se centra en el an√°lisis, asumiendo que una aplicaci√≥n instrumentada ya est√° generando datos.

### ARRANGE (Preparaci√≥n)

*   Asumimos que existe un servicio de Cloud Run con instrumentaci√≥n para Trace y Logging.
*   Asumimos que este servicio se conecta a una instancia de Cloud SQL (PostgreSQL).
*   En la tabla `products` de la base de datos, nos aseguramos de que **NO** haya un √≠ndice en la columna por la que buscamos.

### ACT (An√°lisis en la Consola)

1.  **Ir a Cloud Monitoring > Metrics Explorer:**
    *   **Recurso:** `Cloud Run Revision`
    *   **M√©trica:** `Request Latencies` (`run.googleapis.com/request_latencies`)
    *   **Agregaci√≥n:** `95th percentile`
    *   **Visualizaci√≥n:** Observar el gr√°fico y localizar un pico de latencia.

2.  **Saltar a Cloud Trace:**
    *   En el gr√°fico de Monitoring, selecciona un punto de datos en el pico de latencia y haz clic en "Ver Trazas".
    *   En la lista de trazas, busca una con una duraci√≥n alta (ej. > 5000 ms).
    *   Haz clic en ella para ver el diagrama de cascada (waterfall).
    *   Identifica el span m√°s largo. En nuestro caso, ser√° una llamada a la base de datos.

3.  **Correlacionar con Cloud Logging:**
    *   Dentro de la vista de la traza, busca el panel de "Logs" o el bot√≥n "Mostrar Logs".
    *   Esto te llevar√° al Logs Explorer con el filtro de traza ya aplicado.
    *   Busca en los logs de la aplicaci√≥n mensajes relacionados con la ejecuci√≥n de consultas SQL. Un buen log de aplicaci√≥n incluir√≠a la consulta ejecutada y el tiempo que tard√≥.

### ASSERT (Confirmaci√≥n y Soluci√≥n)

1.  **Confirmaci√≥n:** Los pasos anteriores nos confirman que el problema es una consulta lenta a la base de datos.
2.  **Soluci√≥n Propuesta:** A√±adir un √≠ndice a la columna `id` de la tabla `products`.
    ```sql
    CREATE INDEX idx_products_id ON products(id);
    ```
3.  **Verificaci√≥n Post-Implementaci√≥n:** Despu√©s de aplicar el √≠ndice, se vuelve a observar la m√©trica de latencia en Cloud Monitoring. El pico de latencia deber√≠a haber desaparecido, y las nuevas trazas para la misma petici√≥n deber√≠an mostrar una duraci√≥n de milisegundos en lugar de segundos.

---

## üí° Lecciones Aprendidas

*   **El Flujo de Observabilidad es un Embudo:** Se empieza con una vista de 10,000 pies (Monitoring) para saber *qu√©* pasa, se baja a 1,000 pies (Trace) para saber *d√≥nde*, y se aterriza a nivel del suelo (Logging) para entender el *porqu√©*.
*   **La Instrumentaci√≥n es Requisito Previo:** Nada de esto funciona si la aplicaci√≥n no est√° correctamente instrumentada. Las bibliotecas de cliente de Google Cloud para logging y tracing facilitan enormemente este trabajo.
*   **Los Logs Deben Tener Contexto:** Un buen log no solo dice "Error", sino que incluye el ID de la petici√≥n, el ID del usuario, la traza y cualquier variable relevante. Esto es lo que permite conectar un problema a una causa espec√≠fica.

---

## üßæ Resumen

El stack de observabilidad de GCP proporciona un flujo de trabajo potente y cohesionado para el an√°lisis de causa ra√≠z. Pasando sistem√°ticamente de las m√©tricas agregadas en Cloud Monitoring, a las trazas distribuidas en Cloud Trace, y finalmente a los logs detallados en Cloud Logging, los ingenieros pueden diagnosticar eficientemente problemas de rendimiento complejos, aislando la causa desde un s√≠ntoma a nivel de sistema hasta una l√≠nea de c√≥digo o una consulta de base de datos espec√≠fica.

---

## ‚úçÔ∏è Firma

**Marco - DevSecOps Kulture**  
*The Artisan Path*  
üìß Contacto: [markitos.es.info@gmail.com](mailto:markitos.es.info@gmail.com)  
üêô GitHub: [https://github.com/markitos-public](https://github.com/markitos-public)

---

[‚¨ÜÔ∏è **Volver arriba**](#-caso-pr√°ctico-rca-de-lentitud-en-app-con-el-stack-de-observabilidad)
