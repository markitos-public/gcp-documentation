# üïµÔ∏è‚Äç‚ôÇÔ∏è Caso Pr√°ctico: Troubleshooting de Fallos de Comunicaci√≥n en un MIG

## üéØ Objetivo

Este documento describe un flujo de trabajo sistem√°tico para diagnosticar un problema com√∫n y complejo en una arquitectura de microservicios: un fallo intermitente de comunicaci√≥n. Aprenderemos a usar el stack de observabilidad de Google Cloud (Monitoring, Trace, Logging y Debugger) de forma integrada para ir desde la detecci√≥n del s√≠ntoma hasta la identificaci√≥n de la causa ra√≠z, aplicando una metodolog√≠a de "embudo" (de lo general a lo espec√≠fico).

---

## üèóÔ∏è Escenario

### Arquitectura

*   **Servicio A:** Un microservicio frontend (ej. en App Engine) que act√∫a como cliente.
*   **Balanceador de Carga Externo:** Un balanceador de carga global (HTTP/S) que recibe las peticiones y las distribuye a los backends.
*   **Servicio B (MIG):** Un microservicio backend desplegado en un Grupo de Instancias Administrado (MIG) de Compute Engine. Este servicio se autoescala seg√∫n la carga y es el responsable de procesar la l√≥gica de negocio.

![Arquitectura del Escenario](https://storage.googleapis.com/gcp-prod-images/diagrams/troubleshooting-scenario.png)  
*(Diagrama conceptual de la arquitectura)*

### El Problema

El equipo de soporte informa que los usuarios del **Servicio A** est√°n experimentando errores intermitentes. Las quejas incluyen mensajes de error gen√©ricos y tiempos de espera agotados. El equipo de desarrollo del **Servicio A** confirma que su servicio parece estar bien, pero las llamadas que realiza al **Servicio B** a menudo fallan con errores **HTTP 502 (Bad Gateway)**.

**Nuestra misi√≥n:** Encontrar la causa ra√≠z del problema.

---

## üõ†Ô∏è Metodolog√≠a: El Embudo de Observabilidad

Abordaremos el problema como un SRE, haciendo preguntas cada vez m√°s espec√≠ficas y usando la herramienta adecuada para responder a cada una.

### Paso 1: ¬øEst√° Roto? - La Visi√≥n General con Cloud Monitoring

**Pregunta clave:** ¬øEs un problema real y cuantificable o son quejas aisladas? ¬øCu√°l es el impacto?

**Acciones:**

1.  **Revisar el Dashboard del Balanceador de Carga:** Es nuestro punto de entrada. En la consola de GCP, vamos a `Monitoring > Dashboards > GCP > Load Balancing`. Nos centramos en el panel "Backend Request Errors".
    *   **Hallazgo:** Observamos un aumento significativo en las respuestas con c√≥digo `5xx`, espec√≠ficamente `502`. Esto confirma que el problema es real y est√° ocurriendo entre el balanceador y nuestros backends (el MIG).

2.  **Revisar las M√©tricas del MIG:** En el mismo dashboard o en `Compute Engine > Instance groups`, miramos las m√©tricas del MIG que aloja al **Servicio B**.
    *   **Hallazgo:** El uso de CPU y de red no parece estar en niveles de saturaci√≥n. El n√∫mero de instancias es estable. Esto sugiere que el problema podr√≠a no ser de sobrecarga, sino un error en la aplicaci√≥n misma.

3.  **Verificar el Uptime Check:** Si tenemos un Uptime Check configurado contra la IP del balanceador, revisamos su estado.
    *   **Hallazgo:** El Uptime Check tambi√©n muestra fallos intermitentes, confirmando que el servicio no est√° disponible de forma consistente desde la perspectiva de un cliente externo.

**Conclusi√≥n del Paso 1:** Cloud Monitoring nos ha confirmado que existe un problema real de errores `502` originado en la comunicaci√≥n con los backends del MIG. El problema no parece ser de saturaci√≥n de recursos. Necesitamos profundizar m√°s.

---

### Paso 2: ¬øD√≥nde est√° Roto? - El Viaje de la Solicitud con Cloud Trace

**Pregunta clave:** El balanceador no puede hablar con el backend, pero ¬øen qu√© punto exacto de la comunicaci√≥n se produce el fallo?

**Acciones:**

1.  **Filtrar Trazas Fallidas:** En la consola, vamos a `Trace > Trace list`. Aplicamos un filtro para encontrar las solicitudes que resultaron en un error 502. El filtro ser√≠a: `http.status_code:502`.

2.  **Analizar el Gr√°fico de Cascada (Waterfall Graph):** Seleccionamos una de las trazas fallidas para ver su detalle.
    *   **Hallazgo:** El gr√°fico de cascada nos muestra claramente el viaje: vemos un primer span largo que representa la llamada desde el **Servicio A** hasta el **Servicio B**. Vemos que este span principal tiene un error. Dentro de √©l, vemos un sub-span que representa la l√≥gica interna del **Servicio B**, y es este el que est√° marcado en rojo y tiene el error.

**Conclusi√≥n del Paso 2:** Cloud Trace ha sido un bistur√≠. Nos ha permitido descartar problemas de red entre el cliente y el balanceador. El problema est√° definitivamente **dentro del c√≥digo o el entorno de las instancias del Servicio B**.

---

### Paso 3: ¬øPor qu√© est√° Roto? - La Evidencia en Cloud Logging

**Pregunta clave:** Sabemos que el c√≥digo del **Servicio B** est√° fallando. ¬øQu√© error espec√≠fico est√° ocurriendo?

**Acciones:**

1.  **Navegar desde Trace a Logging:** La forma m√°s r√°pida es usar la integraci√≥n. Desde el span fallido en Cloud Trace, hacemos clic en "Show Logs". Esto nos lleva directamente al Log Explorer, filtrando los logs exactos que se produjeron durante esa traza.

2.  **Buscar Logs de Error:** En el Log Explorer, buscamos logs con `severity=ERROR` o `CRITICAL`.
    *   **Hallazgo:** Encontramos varias entradas de error con un stack trace de la aplicaci√≥n. Leyendo el stack trace, vemos un mensaje claro: `FATAL: password authentication failed for user "app_user"` o `Error: connect ECONNREFUSED 127.0.0.1:5432`.

**Conclusi√≥n del Paso 3:** Cloud Logging nos ha dado la "pistola humeante". El **Servicio B** no puede conectarse a su base de datos PostgreSQL. El error no es de red, sino de autenticaci√≥n o conexi√≥n a la base de datos.

---

### Paso 4: ¬øCu√°l era el Estado del C√≥digo? - La Inspecci√≥n con Cloud Debugger

**Pregunta clave:** El log dice que la conexi√≥n a la base de datos falla. Si el log no fuera tan claro, ¬øc√≥mo podr√≠amos verificar qu√© credenciales o configuraci√≥n de conexi√≥n se est√°n usando en el momento del fallo?

**Acciones:**

1.  **Establecer una Snapshot:** En la consola, vamos a `Debugger`. Navegamos hasta el c√≥digo fuente del **Servicio B**, espec√≠ficamente a la funci√≥n que crea la conexi√≥n con la base de datos.

2.  **Colocar la Snapshot:** Hacemos clic en el n√∫mero de la l√≠nea justo antes de donde se intenta la conexi√≥n para establecer una snapshot.

3.  **Analizar la Captura:** Esperamos a que una nueva solicitud active la snapshot. Una vez capturada, el panel de Debugger nos muestra todas las variables locales en ese momento.
    *   **Hallazgo:** Al inspeccionar las variables, descubrimos que la variable `db_password` que se est√° pasando al conector de la base de datos es `null` o est√° vac√≠a. Tambi√©n podr√≠amos ver que la variable `db_host` apunta a `localhost` cuando deber√≠a apuntar a una IP de Cloud SQL.

**Conclusi√≥n del Paso 4:** Cloud Debugger nos permite confirmar la hip√≥tesis de los logs de una manera irrefutable. Vemos el estado exacto de las variables en el momento del fallo, confirmando que se est√° intentando una conexi√≥n con credenciales incorrectas.

---

## üèÅ Conclusi√≥n Final y Causa Ra√≠z

El viaje ha terminado. Hemos pasado de un vago "la web falla" a una causa ra√≠z precisa:

*   **S√≠ntoma:** Usuarios reciben errores `502` (detectado por **Monitoring**).
*   **Localizaci√≥n:** El error se origina en el **Servicio B** al ser llamado por el balanceador (identificado por **Trace**).
*   **Error Espec√≠fico:** La aplicaci√≥n en el **Servicio B** no puede conectarse a su base de datos (encontrado en **Logging**).
*   **Causa Ra√≠z:** La variable que contiene la contrase√±a de la base de datos es nula en el momento de la conexi√≥n (verificado con **Debugger**). Probablemente, un cambio reciente en la configuraci√≥n o en la forma de leer los secrets ha introducido este bug.

Con esta informaci√≥n, el equipo de desarrollo puede solucionar el problema de forma r√°pida y precisa.

### Resumen del Flujo de Trabajo

1.  **Monitoring:** ¬øHay un problema? (Dashboards, Alertas, Uptime Checks).
2.  **Trace:** ¬øD√≥nde est√° el problema? (An√°lisis de latencia y errores en el grafo de la solicitud).
3.  **Logging:** ¬øPor qu√© ocurre el problema? (B√∫squeda de errores y stack traces).
4.  **Debugger/Profiler:** ¬øCu√°l es el estado exacto del c√≥digo? (Inspecci√≥n de variables o an√°lisis de rendimiento a nivel de funci√≥n).
