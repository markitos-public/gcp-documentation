# culture Monitorizaci√≥n y Alerting en SRE

## üìë √çndice
* [üß≠ Descripci√≥n](#-descripci√≥n)
* [üìò Detalles](#-detalles)
* [üíª Laboratorio Pr√°ctico (CLI-TDD)](#-laboratorio-pr√°ctico-cli-tdd)
* [üí° Lecciones Aprendidas](#-lecciones-aprendidas)
* [‚ö†Ô∏è Errores y Confusiones Comunes](#Ô∏è-errores-y-confusiones-comunes)
* [üéØ Tips de Examen](#-tips-de-examen)
* [üßæ Resumen](#-resumen)
* [‚úçÔ∏è Firma](#-firma)
* [‚¨ÜÔ∏è Volver arriba](#culture-monitorizaci√≥n-y-alerting-en-sre)

---

## üß≠ Descripci√≥n

La monitorizaci√≥n en SRE va m√°s all√° de simplemente observar gr√°ficos de CPU. Se centra en medir lo que realmente importa: la experiencia del usuario. El alerting (la generaci√≥n de alertas) se trata con sumo cuidado para que cada alerta sea significativa y accionable, evitando la fatiga por alertas. El enfoque de SRE es pasar de un modelo reactivo (alertar cuando algo se rompe) a uno proactivo (alertar cuando el presupuesto de error est√° en peligro).

---

## üìò Detalles

### Los Cuatro Golden Signals

Para cualquier servicio, SRE recomienda monitorizar cuatro se√±ales fundamentales que indican la salud del sistema desde la perspectiva del usuario:

1.  **Latencia (Latency):** El tiempo que tarda en servirse una petici√≥n. Es crucial distinguir entre la latencia de las peticiones exitosas y las fallidas.
2.  **Tr√°fico (Traffic):** La cantidad de demanda que est√° soportando el sistema, medida en una m√©trica apropiada para el servicio (ej. peticiones HTTP por segundo).
3.  **Errores (Errors):** La tasa de peticiones que fallan, ya sea expl√≠citamente (ej. c√≥digos HTTP 500) o impl√≠citamente (ej. una respuesta 200 con contenido incorrecto).
4.  **Saturaci√≥n (Saturation):** Cu√°n "lleno" est√° el servicio. Es una medida de la utilizaci√≥n de los recursos m√°s limitados (ej. uso de memoria, I/O de disco). La saturaci√≥n predice problemas futuros de rendimiento.

### Filosof√≠a de Alerting

El objetivo de las alertas en SRE no es notificar cada anomal√≠a, sino solo aquellas que requieren intervenci√≥n humana para solucionarse. Una buena alerta debe ser:

*   **Urgente:** Requiere atenci√≥n inmediata.
*   **Accionable:** Especifica claramente qu√© se debe hacer.
*   **Importante:** Indica un impacto real en el servicio.

SRE se enfoca en alertar sobre la **tasa de consumo del presupuesto de error (Error Budget Burn Rate)**. Si el servicio empieza a consumir el presupuesto de error a una velocidad que amenaza con agotarlo antes de que termine el periodo del SLO, se dispara una alerta. Esto permite al equipo actuar *antes* de que el SLO se viole.

```bash
# Ejemplo ilustrativo: Listar las pol√≠ticas de alertas en tu proyecto.
# Estas pol√≠ticas definen cu√°ndo se debe notificar a un humano.
gcloud alpha monitoring policies list
```

---

## üíª Laboratorio Pr√°ctico (CLI-TDD)

### üìã Escenario 1: Crear una Pol√≠tica de Alerta basada en M√©tricas
**Contexto:** Crearemos una pol√≠tica de alerta que nos notifique si el uso de CPU de una instancia de Compute Engine supera el 80% durante m√°s de 5 minutos. Esto es un indicador de saturaci√≥n y puede predecir problemas de latencia.

#### ARRANGE (Preparaci√≥n del laboratorio)
```bash
# Habilitar APIs necesarias
gcloud services enable monitoring.googleapis.com compute.googleapis.com --project=$PROJECT_ID

# Variables de entorno
export PROJECT_ID=$(gcloud config get-value project)
export VM_NAME="vm-to-monitor"
export ALERT_POLICY_NAME="high-cpu-usage-policy"

# Crear una VM de prueba para monitorizar
gcloud compute instances create $VM_NAME --zone=europe-west1-b --machine-type=e2-micro

# Crear un canal de notificaci√≥n (ej. por email, requiere configuraci√≥n en la consola)
# Para este lab, asumimos que ya existe un canal o lo creamos manualmente.
# export NOTIFICATION_CHANNEL_ID=$(gcloud alpha monitoring channels list --format='value(name)')
```

#### ACT (Implementaci√≥n del escenario)
*Creamos una pol√≠tica de alerta que se dispara cuando la utilizaci√≥n de la CPU supera un umbral.*
```bash
# Crear la pol√≠tica de alerta desde un fichero de configuraci√≥n
cat <<EOT > alert-policy.json
{
  "displayName": "Uso de CPU elevado en VM de prueba",
  "combiner": "OR",
  "conditions": [
    {
      "displayName": "Uso de CPU > 80% durante 5 minutos",
      "conditionThreshold": {
        "filter": "metric.type=\"compute.googleapis.com/instance/cpu/utilization\" resource.type=\"gce_instance\" resource.label.instance_id=\"$(gcloud compute instances describe $VM_NAME --zone=europe-west1-b --format='value(id)')\"",
        "comparison": "COMPARISON_GT",
        "thresholdValue": 0.8,
        "duration": "300s",
        "trigger": {
          "count": 1
        }
      }
    }
  ]
}
EOT

gcloud alpha monitoring policies create --policy-from-file=alert-policy.json
```

#### ASSERT (Verificaci√≥n de funcionalidades)
*Verificamos que la pol√≠tica de alerta ha sido creada correctamente.*
```bash
# Listar las pol√≠ticas de alerta y filtrar por nuestro nombre
gcloud alpha monitoring policies list --filter="displayName='Uso de CPU elevado en VM de prueba'"
```

#### CLEANUP (Limpieza de recursos)
```bash
# Eliminar la pol√≠tica de alerta y la VM
export POLICY_ID=$(gcloud alpha monitoring policies list --filter="displayName='Uso de CPU elevado en VM de prueba'" --format="value(name)")
gcloud alpha monitoring policies delete $POLICY_ID --quiet
gcloud compute instances delete $VM_NAME --zone=europe-west1-b --quiet

echo "‚úÖ Laboratorio completado y recursos eliminados."
```

---

## üí° Lecciones Aprendidas

*   **Monitoriza s√≠ntomas, no causas:** El uso de CPU alto (causa) no es tan importante como una latencia elevada (s√≠ntoma que afecta al usuario).
*   **Las alertas deben ser accionables:** Si una alerta no te dice qu√© hacer, es ruido. Cada alerta debe estar ligada a un runbook o a un procedimiento claro.
*   **Alerta sobre el burn rate del presupuesto de error:** Es el m√©todo m√°s sofisticado. Te permite actuar antes de violar tu SLO, pero requiere una buena definici√≥n de SLIs.

---

## ‚ö†Ô∏è Errores y Confusiones Comunes

*   **Fatiga por alertas (Alert Fatigue):** Crear demasiadas alertas de baja prioridad que el equipo de guardia empieza a ignorar. Es un problema grave que puede enmascarar incidentes reales.
*   **Monitorizar todo:** Recopilar miles de m√©tricas sin un prop√≥sito claro solo a√±ade ruido y complejidad. C√©ntrate en los Golden Signals.
*   **Alertar sobre umbrales est√°ticos sin contexto:** Una alerta de "CPU > 90%" puede ser normal para un job de batch, pero cr√≠tica para un servidor web interactivo. Las alertas deben tener contexto.

---

## üéØ Tips de Examen

*   **Conoce los 4 Golden Signals:** Latencia, Tr√°fico, Errores y Saturaci√≥n. El examen puede pedirte que identifiques cu√°l es el m√°s apropiado para un escenario dado.
*   **Diferencia entre White-box y Black-box monitoring:** White-box es monitorizar el interior de tu sistema (m√©tricas de la JVM, logs). Black-box es monitorizar desde fuera, como lo har√≠a un usuario (probes de disponibilidad, tests sint√©ticos).
*   **El objetivo del alerting en SRE:** No es notificar cada problema, sino solo aquellos que requieren intervenci√≥n humana y amenazan el SLO.

---

## üßæ Resumen

La monitorizaci√≥n y el alerting en SRE son disciplinas enfocadas en la experiencia del usuario y la gesti√≥n proactiva de la fiabilidad. Al centrarse en los Cuatro Golden Signals y en alertar sobre la tasa de consumo del presupuesto de error, los equipos SRE evitan la fatiga por alertas y dedican su tiempo a solucionar problemas que realmente importan, antes de que impacten significativamente en el servicio.

---

## ‚úçÔ∏è Firma

**Marco - DevSecOps Kulture**  
*The Artisan Path*  
üìß Contacto: [markitos.es.info@gmail.com](mailto:markitos.es.info@gmail.com)  
üêô GitHub: [https://github.com/markitos-public](https://github.com/markitos-public)

---

[‚¨ÜÔ∏è **Volver arriba**](#culture-monitorizaci√≥n-y-alerting-en-sre)
